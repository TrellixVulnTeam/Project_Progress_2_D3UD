# Object detection-based Scene Classification

## Instructions

**Notice: all files are under progress and subjected to change**

- The Code file contains the code progress implemented upto 27th of July and is subjected to change

- The Dataset file contains the dataset generated before 16th of July and the is subjected to change

- The Trained model file contains the trained model for classification and object detection.

## Project Description

The main aim of the project is to design a real-time scene classification model to classify an image based on the objects detected in the image. The model is trained on all the possible combinations of objects that can appear in a frame/image to constitute a specific class (for example, if the objects detected in a frame/image are an oven, refrigerator, microwave and a sink, the model can determine the class of the image as kitchen).
The project can be divided into two parts,

- Detection of objects in an image
- Classification of the image based on the detected object

The object detection model is trained using a dataset of images of various indoor scenes (classes), with each having annotations/labels of various objects in the image. The dataset is trained with a pre-trained object detection model, Mobilenet v2. It was chosen for its high speed and fast training, which makes it suitable for real-time object detection.  

To classify an image based on the objects detected in a frame/image, the annotations generated for each frame by the object detector are trained on a classification model. The annotations generated by the images will be used to generate a tabular dataset containing all the information about the number of occurrences of each detected object and the class that it belongs to. The classification models that will be used are Feed-forward Neural network, K-Nearest Neighbors Classifier and Decision tree algorithm. Based on the accuracy and speed of the models, one model will be chosen for classification.

While testing the model, the annotations or labels detected by the object detection model are fed to the classification model to determine the class of the image.

![](https://github.com/osman-95/Project_Progress_2/blob/master/ReadMe_img/Md.jpg)


Before Elaborating the above sections, a quick overview on the file hierarchy of the main files are shown below
         
```   
         
└── Final_Model (Note: In progress last updated on 10th of july)
     └── Code
     └── Dataset
     └── Trained_models

 ```       

## Description of the dataset

In this project, the object detector was trained with a set of indoor scene images and the classification model was trained on a tabular CSV data containing the occurrence information of the detected object and their respective geometrical coordinate’s information.  

### Object Detection data description

The object detector was trained to detect  25 different indoor objects using a pre-trained coco mobilenetv2 model from [TensorFlow API]().  The 25 classes are listed below.
- Oven
- Refrigerator
- Sink
- Rangehood
- Bathtub
- Mirror
- Toilet seat
- Towel
- Bed
- Dresser cabinet
- Night table
- Table lamp
- Bowling ball
- Bowling pins
- Bowling rack
- Tv screen
- Dining table lamp
- Dining table
- Vase
- Chair	
- Computer
- Desk
- Keyboard
- Monitor
- Printer

Throughout the project, the model was trained on various image sets of indoor images 
- Dataset-1: 600 images of indoor scenes 
- Dataset-2: 1500 images of indoor scenes and objects
- Dataset-3: 2500 images of indoor scenes and objects

The dataset was created by MIT, and was obtained from [Kaggle](https://www.kaggle.com/itsahmad/indoor-scenes-cvpr-2019?). It consists of 67 categories/classes and a total of 15,620 images of a variety of indoor scenes such as classrooms, restaurants, bars, bakeries, bathrooms, libraries, bedrooms, airports, etc. The number of images in each category varies, but there are at least 100 images per category and all the images are in .jpg format. 

![Dataset](https://raw.githubusercontent.com/osman-95/Project_Prog/master/ReadMe_img/Capture1.PNG)

Only 6 classes were chosen to be worked on, with each containing 100 images. The decision to choose only 6 classes was made due to time constraints and hardware limitations. The 6 classes chosen were bedroom, bathroom, dining room, kitchen, bowling center, and office.

 ![Selected categories](https://github.com/osman-95/Project_Progress_2/blob/master/ReadMe_img/Capture12121.PNG)

I labelled the dataset manually using [VoTT software](https://github.com/microsoft/VoTT), with each class having at least 4 different types of annotations/labels. 

 ![](https://github.com/osman-95/Project_Progress_2/blob/master/ReadMe_img/Capture21211.PNG)
 
 ### Classification data description
 
 The classification dataset was built from the annotation CSV file of the Dataset 1 (600 images of indoor scenes). The dataset contains information about the occurrence of each object in an image and their geometrical coordinate’s information of these objects mapping it with their respective class as an output. Two datasets were built from the annotation CSV file: 
- Classification dataset 1- Contains the information about the occurrence of each object in an image
- Classification dataset 2- Contains the information about the occurrence of each object in an image in addition to their geometrical positional coordinates

## Training Process

The object detection and classification models were trained separately in the training stage and the final models for both sections(object detection and classification) were chosen based on the individual evaluation of each model with their respective test data.

### Object detection training process

The 3 datasets built  for object detection throughout the course of the project  were trained on pretrained Coco  MobileNetV2 model in different configurations:
- Model 1: Dataset-1 only (600 images)
- Model 2: Dataset-2 only (1500 images)
- Model 3: Dataset-3 only (2500 images)
- Model 4: Dataset-1 cascaded with Dataset-2 (600x1500)
- Model 5: Dataset-2 cascaded with Dataset-3 (1500x2500)
- Model 6: Dataset-1 cascaded with Dataset-2 and the model built is cascaded with Dataset-3 (600x1500x2500)

### Classification training process

The two classification datasets (1- annotations only and 2-annotation with geometrical coordinate’s information)discussed earlier were used for training the classification model. The different algorithms were used for training are shown below:
•	Feed-forward neural network
•	K-nearest neighbors
•	Decision tree algorithm 


The complete training process is demonstrated in one diagram shown below.

![](https://github.com/osman-95/Project_Progress_2/blob/master/ReadMe_img/Model%20(3).jpg)

## Results

The complete model was evaluated under two sections: object detection section and classification section. The two sections were evaluated separately as the process of evaluation of each section is different. 

### Object detection evaluation

The object detector was evaluated based on mean Average precision and mean average recall values measured over multiple intersection of union and for various object sizes.

![](https://github.com/osman-95/Project_Progress_2/blob/master/ReadMe_img/tb1.PNG)
**Table 1: Mean average precision and average recall for various models**

### Classification evaluation

The classification model was evaluated using precision, recall and F1 metrics. In addition to that, the processing time of each frame or the speed of the model was considered since the model needs to be implemented in real-time

![](https://github.com/osman-95/Project_Progress_2/blob/master/ReadMe_img/tb2.PNG)
**Table 2: Accuracy, precision and recall for all the 3 models tested on the two datasets.**

The confusion matrix of all the 3 models is shown below.  

![](https://github.com/osman-95/Project_Progress_2/blob/master/ReadMe_img/Capture546.PNG)
![](https://github.com/osman-95/Project_Progress_2/blob/master/ReadMe_img/Capture32.PNG)
![](https://github.com/osman-95/Project_Progress_2/blob/master/ReadMe_img/Capture56.PNG)


Since all models showed a very good performance with accuracy values above 90%, the main factor for choosing the final model was based on the speed of the model. The speed of processing time of the models was evaluated by calculating the time taken to process a single frame. The results of the speed test are shown below. 

![](https://github.com/osman-95/Project_Progress_2/blob/master/ReadMe_img/tb3.PNG)
**Table 3: The accuracy and speed of each algorithm**


### Sample output of the complete model

## Conclusion

Our model showed an acceptable performance with most for the inaccuracy occurring in the object detection stage. The object detection showed an accuracy of about 59% with an average recall of about 71%. The model was able to detect a significant about of large and medium objects (APL=61%, APM=41% ) but struggled in detecting small objects (APS= 0.10). The classification part on the other hand showed an outstanding performance on all tested models with no accuracy below 92%. The selection of the classification model was completely based on the processing time or execution speed of the model. The decision tree showed a high speed of about 0.00842 seconds per frame making it reliable for real-time processing. The project is in its initial stages and needs further research, improvement, training and optimization to produce a finished model to be applicable for general use.  
